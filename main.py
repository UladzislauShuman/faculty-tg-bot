import argparse
import yaml
import sys
import os
import shutil
import asyncio
import time
from dotenv import load_dotenv

from langchain_huggingface import HuggingFaceEmbeddings
# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∏–º–ø–æ—Ä—Ç –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π LangChain
from langchain.evaluation import load_evaluator

# –¢–≤–æ–∏ –º–æ–¥—É–ª–∏
from src.di_containers import Container
from src.pipelines.indexing.pipeline import run_indexing
from src.util.yaml_parser import load_qa_test_set

# --- –ö–û–ù–°–¢–ê–ù–¢–´ ---
MAX_CONTEXT_CHARS = 12000
HIT_RATE_THRESHOLD = 0.4


# --- –£–¢–ò–õ–ò–¢–´ ---

def manage_db_state_for_test(config_data: dict, force_reindex: bool):
  """–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø—É—Ç—è–º–∏ –¢–û–õ–¨–ö–û –¥–ª—è –¢–ï–°–¢–û–í–û–ì–û —Ä–µ–∂–∏–º–∞."""
  db_path = config_data['retrievers']['vector_store']['db_path']
  bm25_path = config_data['retrievers']['bm25']['index_path']

  exists = os.path.exists(db_path) and os.path.exists(bm25_path)

  if force_reindex:
    if exists:
      print(f"‚ôªÔ∏è  [TEST] –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤: {db_path}...")
      try:
        shutil.rmtree(db_path)
        if os.path.exists(bm25_path): os.remove(bm25_path)
      except OSError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª—ã –ë–î –∑–∞–Ω—è—Ç—ã. {e}")
        sys.exit(1)
    return True

  if not exists:
    print(f"‚ö†Ô∏è  [TEST] –ò–Ω–¥–µ–∫—Å—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ.")
    return True

  print(f"‚úÖ  [TEST] –ù–∞–π–¥–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏–Ω–¥–µ–∫—Å—ã: {db_path}")
  return False


def simple_ru_stem(word: str) -> str:
  word = word.lower().strip('.,!?;:()')
  for ending in ['–∞–º–∏', '—è–º–∏', '–æ–≤', '–µ–≤', '–µ–π', '–∞–º', '—è–º', '–∞—Ö', '—è—Ö', '—É—é',
                 '—é—é', '–∞', '—è', '–æ', '–µ', '—ã', '–∏', '—É', '—é']:
    if word.endswith(ending) and len(word) > len(ending) + 2:
      return word[:-len(ending)]
  return word


def calculate_hit_rate(qa_pair, retrieved_docs):
  reference_words = [simple_ru_stem(w) for w in qa_pair['answer'].split() if
                     len(w) > 3]
  if not reference_words: return 0

  for doc in retrieved_docs:
    content = doc.page_content.lower()
    found_count = 0
    for stem in reference_words:
      if stem in content:
        found_count += 1

    if found_count / len(reference_words) >= HIT_RATE_THRESHOLD:
      return 1
  return 0


def save_test_report(results, args, avg_metrics, config):
  """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –≤ Markdown."""
  output_dir = config['paths']['output_dir']
  filename = f"report_{args.chunker}_{args.index_mode}.md"
  report_path = os.path.join(output_dir, filename)
  os.makedirs(output_dir, exist_ok=True)

  with open(report_path, "w", encoding="utf-8") as f:
    f.write(f"# üìä –û—Ç—á–µ—Ç –æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ RAG\n")
    f.write(f"**–î–∞—Ç–∞:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(
      f"**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:** Chunker=`{args.chunker}`, Mode=`{args.index_mode}`, Retriever=`{args.retriever}`\n\n")

    f.write("## üìà –°–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n")
    f.write(f"- **Hit Rate (Retrieval):** {avg_metrics['hit']:.2%}\n")
    f.write(f"- **Similarity (Generation):** {avg_metrics['sim']:.4f}\n")
    f.write(f"- **Avg Latency:** {avg_metrics['lat']:.2f}s\n\n")

    f.write("## üìù –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º\n")
    for i, res in enumerate(results, 1):
      icon = "‚úÖ" if res['hit'] else "‚ùå"
      f.write(f"### {i}. {res['q']}\n")
      f.write(f"- **–≠—Ç–∞–ª–æ–Ω:** {res['ref']}\n")
      f.write(f"- **–û—Ç–≤–µ—Ç –ë–æ—Ç–∞:** {res['a']}\n")
      f.write(
        f"- **–ú–µ—Ç—Ä–∏–∫–∏:** {icon} Hit={res['hit']} | Sim={res['score']:.4f} | Time={res['latency']:.2f}s\n")
      f.write("\n---\n")

  print(f"\nüìÑ –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {report_path}")


# --- PIPELINE –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ---

async def run_full_test_pipeline(args, container, config_data):
  # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ß–∞–Ω–∫–µ—Ä–∞
  processor_name = f"{args.chunker}_processor"
  try:
    processor_provider = getattr(container, processor_name)
    container.data_processor.override(processor_provider)
  except AttributeError:
    sys.exit(f"‚ùå –û—à–∏–±–∫–∞: –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä '{processor_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ DI.")

  # 2. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
  if args.need_index:
    print(f"\nüèóÔ∏è  –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ ({args.chunker})...")
    processor = container.data_processor()
    run_indexing(config=config_data, processor=processor, mode=args.index_mode)

  # 3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
  print(f"\nüß™ –ó–ê–ü–£–°–ö –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø (Retriever: {args.retriever})...")
  qa_set = load_qa_test_set(config_data['paths']['qa_test_set'])
  if not qa_set: sys.exit("QA set empty")

  retrieval_chain = container.retrieval_chain()
  generation_chain = container.generation_chain()

  eval_emb = HuggingFaceEmbeddings(
      model_name=config_data.get('evaluation_model', {}).get('name',
                                                             'cointegrated/rubert-tiny2'),
      model_kwargs={'device': 'cpu'}
  )
  evaluator = load_evaluator("embedding_distance", embeddings=eval_emb)

  # –®–∞–≥ 1: Batch Retrieval
  print(f"üîé [1/2] –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è {len(qa_set)} –≤–æ–ø—Ä–æ—Å–æ–≤...")
  retrieval_tasks = [retrieval_chain.ainvoke(qa['question']) for qa in qa_set]
  retrieved_docs_batch = await asyncio.gather(*retrieval_tasks)

  # –®–∞–≥ 2: Sequential Generation
  print(f"ü§ñ [2/2] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)...")
  results = []
  total_hit_rate = 0

  for i, (qa, docs) in enumerate(zip(qa_set, retrieved_docs_batch)):
    hit = calculate_hit_rate(qa, docs)
    total_hit_rate += hit
    print(
      f"[{i + 1}/{len(qa_set)}] {qa['question'][:40]}... (Docs: {len(docs)}, Hit: {hit})")

    safe_docs = []
    current_chars = 0
    for d in docs:
      doc_len = len(d.page_content)
      if current_chars + doc_len < MAX_CONTEXT_CHARS:
        safe_docs.append(d)
        current_chars += doc_len
      else:
        break

    full_context = "\n\n".join(
        [f"–ò—Å—Ç–æ—á–Ω–∏–∫: {d.metadata.get('source')}\n{d.page_content}" for d in
         safe_docs])

    start_time = time.time()
    try:
      response = await generation_chain.ainvoke(
          {"context": full_context, "question": qa['question']})
      latency = time.time() - start_time

      dist = \
      evaluator.evaluate_strings(prediction=response, reference=qa['answer'])[
        'score']
      score = 1.0 - dist

      results.append({
        "q": qa['question'],
        "a": response,
        "ref": qa['answer'],  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç—Ç–∞–ª–æ–Ω
        "score": score,
        "latency": latency,
        "hit": hit  # –°–æ—Ö—Ä–∞–Ω—è–µ–º Hit Rate
      })

    except Exception as e:
      print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
      results.append(
          {"q": qa['question'], "a": f"ERROR: {e}", "ref": qa['answer'],
           "score": 0.0, "latency": 0.0, "hit": 0})

  # –û—Ç—á–µ—Ç
  if results:
    avg_metrics = {
      "sim": sum(r['score'] for r in results) / len(results),
      "lat": sum(r['latency'] for r in results) / len(results),
      "hit": total_hit_rate / len(results)
    }

    print("\n" + "=" * 60)
    print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ ({args.chunker} | {args.index_mode})")
    print("=" * 60)
    print(f"‚úÖ Hit Rate (Retrieval): {avg_metrics['hit']:.2%}")
    print(f"‚úÖ Similarity (Generation): {avg_metrics['sim']:.4f}")
    print(f"‚è±Ô∏è  Avg Latency: {avg_metrics['lat']:.2f}s")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
    save_test_report(results, args, avg_metrics, config_data)

  else:
    print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")


# --- MAIN CLI ---

def main():
  load_dotenv()
  parser = argparse.ArgumentParser()
  subparsers = parser.add_subparsers(dest="command")

  # TEST
  test_parser = subparsers.add_parser("test")
  test_parser.add_argument("--chunker", type=str, default="markdown",
                           choices=["markdown", "semantic", "unstructured"])
  test_parser.add_argument("--retriever", type=str, default="hybrid")
  test_parser.add_argument("--index-mode", type=str, default="test",
                           choices=["test", "full"])
  test_parser.add_argument("--force-index", action="store_true")

  # INDEX (Production)
  idx_parser = subparsers.add_parser("index")
  idx_parser.add_argument("mode", nargs="?", default="full",
                          choices=["full", "test"])
  idx_parser.add_argument("--chunker", type=str, default="markdown",
                          choices=["markdown", "semantic", "unstructured"])

  subparsers.add_parser("retrieve").add_argument("-q", "--query")
  subparsers.add_parser("answer").add_argument("-q", "--query")

  args = parser.parse_args()

  try:
    with open('config/config.yaml', 'r') as f:
      config = yaml.safe_load(f)
  except:
    sys.exit("Config missing")

  # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π –¥–ª—è –¢–ï–°–¢–ê
  if args.command == "test":
    base_db = config['retrievers']['vector_store'].get('db_path_base',
                                                       'data/chroma_db')
    config['retrievers']['vector_store'][
      'db_path'] = f"{base_db}_{args.chunker}"

    base_bm25 = config['retrievers']['bm25'].get('index_path_base',
                                                 'data/bm25_index')
    base, ext = os.path.splitext(base_bm25)
    config['retrievers']['bm25']['index_path'] = f"{base}_{args.chunker}{ext}"

    args.need_index = manage_db_state_for_test(config, args.force_index)

  container = Container()
  container.config.from_dict(config)

  if args.command == "test":
    asyncio.run(run_full_test_pipeline(args, container, config))

  elif args.command == "index":
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –ü–†–û–î–ê–ö–®–ù –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (Chunker: {args.chunker})...")

    prod_db_path = config['retrievers']['vector_store']['db_path']
    prod_bm25_path = config['retrievers']['bm25']['index_path']

    print(f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—É—â–µ–π –±–∞–∑—ã: {prod_db_path}")
    try:
      shutil.rmtree(prod_db_path)
      if os.path.exists(prod_bm25_path):
        os.remove(prod_bm25_path)
    except OSError as e:
      sys.exit(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –±–∞–∑—É: {e}\n–û—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –±–æ—Ç–∞ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ.")

    try:
      processor_provider = getattr(container, f"{args.chunker}_processor")
      container.data_processor.override(processor_provider)
    except AttributeError:
      sys.exit(f"‚ùå –û—à–∏–±–∫–∞: –ß–∞–Ω–∫–µ—Ä {args.chunker} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    processor = container.data_processor()
    run_indexing(config, processor, args.mode)

  elif args.command == "retrieve":
    retrieval_step = container.retrieval_chain()
    if args.query:
      print(f"–ü–æ–∏—Å–∫: {args.query}")
      docs = retrieval_step.invoke(args.query)
      for d in docs:
        print(
          f"\n--- {d.metadata.get('title', 'Doc')} ---\n{d.page_content[:200]}...")

  elif args.command == "answer":
    rag_chain = container.rag_chain()
    if args.query:
      print(f"–í–æ–ø—Ä–æ—Å: {args.query}")
      print(rag_chain.invoke(args.query))


if __name__ == "__main__":
  main()