import os
import textwrap
import requests
from bs4 import BeautifulSoup
from langchain_core.documents import Document
from dotenv import load_dotenv
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

load_dotenv()
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "output")
DB_DIR = os.getenv("DB_DIR", "chroma_db")
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "cointegrated/rubert-tiny2")

def load_and_parse_data(url: str):
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 1.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        response = requests.get(url, headers=headers, verify=False)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ URL: {e}")
        return None

    soup = BeautifulSoup(response.text, 'lxml')
    
    # –ë–ª–æ–∫, –≥–¥–µ —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≤–µ—Å—å —Ç–µ–∫—Å—Ç
    # —Å–µ–π—á–∞—Å –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã about –æ–Ω –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫, –Ω–æ –Ω–∞–¥–æ —Ç–æ–≥–¥–∞ 
    # –ø–æ–∑–∂–µ –¥–∞–ª–µ–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∫ –¥–µ–ª–∞ –∏ —É –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    content_element = soup.find('div', id='block-famcs-content')
    
    if not content_element:
        content_element = soup.find('article')
        if not content_element:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")
            return None
        
    elements_to_remove = content_element.find_all([
        'script',           
        'style',          
        'nav',             
        'footer',           
        'header',         
        lambda tag: tag.name == 'div' and 'visually-hidden' in tag.get('class', [])
    ])
    
    for element in elements_to_remove:
        element.decompose()
        
    text = content_element.get_text(separator='\n', strip=True)
    
    clean_text = "\n".join([line for line in text.split('\n') if line.strip()])
    
    metadata = {"source": url, "title": soup.title.string if soup.title else "No title"}
    doc = Document(page_content=clean_text, metadata=metadata)
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∏ –æ—á–∏—â–µ–Ω 1 –¥–æ–∫—É–º–µ–Ω—Ç. –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(clean_text)} —Å–∏–º–≤–æ–ª–æ–≤.")
    return [doc] 

def split_documents(documents: list):
    print("\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        is_separator_regex=False,
    )
    
    chunks = text_splitter.split_documents(documents)
    
    print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤.")
    
    filtered_chunks = [chunk for chunk in chunks if len(chunk.page_content) > 50]
            
    print(f"‚úÖ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(filtered_chunks)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.")
    
    output_file_path = os.path.join(OUTPUT_DIR, "index_pipeline-output.txt")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(output_file_path, "w", encoding="utf-8") as f:
        f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: {documents[0].metadata.get('source', 'N/A')}\n")
        f.write(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(filtered_chunks)}\n")
        f.write("="*80 + "\n\n")

        for i, chunk in enumerate(filtered_chunks):
            f.write(f"--- –ß–ê–ù–ö #{i+1} ---\n")
            f.write(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {chunk.metadata}\n")
            f.write("-" * 20 + "\n")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º textwrap –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            wrapped_text = textwrap.fill(chunk.page_content, width=100)
            f.write(wrapped_text)
            f.write("\n\n" + "="*80 + "\n\n")
            
    print(f"‚úÖ –í—Å–µ —á–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_file_path}")

    return filtered_chunks

def create_and_store_embeddings(chunks: list):
    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    
    model_name = EMBEDDING_MODEL_NAME
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': False}
    embeddings_model = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ embedding-–º–æ–¥–µ–ª–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –≤–µ–∫—Ç–æ—Ä–æ–≤... (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)")
    db = Chroma.from_documents(chunks, embeddings_model, persist_directory=DB_DIR)
    
    print(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ: {DB_DIR}")
    return db

if __name__ == "__main__":
    TARGET_URL = "https://fpmi.bsu.by/about"
    loaded_documents = load_and_parse_data(TARGET_URL)
    if loaded_documents:
        document_chunks = split_documents(loaded_documents)
        vector_database = create_and_store_embeddings(document_chunks)
        print("\nüéâ –í—Å–µ —à–∞–≥–∏ –ö–æ–Ω—Ç—É—Ä–∞ 1 —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")