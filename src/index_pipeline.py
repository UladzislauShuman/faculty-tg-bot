import os
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
OUTPUT_DIR = "output"
DB_DIR = "chroma_db"

def load_data_from_url(url: str):
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
    
    # –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è —Å—Å—ã–ª–∫–∏
    # –í headers –ø–µ—Ä–µ–¥–∞–µ–º User-Agent, —á—Ç–æ–±—ã —Å–∞–π—Ç –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –Ω–∞—Å –∫–∞–∫ –±–æ—Ç–∞
    loader = WebBaseLoader(
        web_path=url,
        header_template={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }, 
        requests_kwargs={"verify": False} 
    )
    
    # –ø–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
    # –†–µ–∑—É–ª—å—Ç–∞—Ç -- —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ Document
    docs = loader.load()
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç(–æ–≤).")
    
    if docs:
        document = docs[0]
        page_content = document.page_content
        metadata = document.metadata
        
        print("\n--- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ---")
        print(metadata)
        
        print("\n--- –ù–∞—á–∞–ª–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤) ---")
        print(page_content[:500])
        print("...")
        
        print("\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤ —Ñ–∞–π–ª")
        
        # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ —Ñ–∞–π–ª
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        file_path = os.path.join(OUTPUT_DIR, "parsed_content.txt")
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {metadata.get('source', 'N/A')}\n")
            f.write(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {metadata.get('title', 'N/A')}\n")
            f.write("="*30 + "\n\n")
            f.write(page_content)
        print(f"‚úÖ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª: {file_path}")
        
    return docs

def split_documents(documents: list):
    print("\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        is_separator_regex=False,
    )
    
    chunks = text_splitter.split_documents(documents)
    
    print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤.")
    
    if chunks:
        print("\n--- –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞ ---")
        print(chunks[0].page_content)
        print("\n--- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞ ---")
        print(chunks[0].metadata)

    return chunks

def create_and_store_embeddings(chunks: list):
    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    
    model_name = "cointegrated/rubert-tiny2"
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': False}
    embeddings_model = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ embedding-–º–æ–¥–µ–ª–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –≤–µ–∫—Ç–æ—Ä–æ–≤... (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)")
    db = Chroma.from_documents(chunks, embeddings_model, persist_directory=DB_DIR)
    
    print(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ: {DB_DIR}")
    return db

if __name__ == "__main__":
    TARGET_URL = "https://fpmi.bsu.by/about"
    loaded_documents = load_data_from_url(TARGET_URL)
    if loaded_documents:
        document_chunks = split_documents(loaded_documents)
        vector_database = create_and_store_embeddings(document_chunks)
        print("\nüéâ –í—Å–µ —à–∞–≥–∏ –ö–æ–Ω—Ç—É—Ä–∞ 1 —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")