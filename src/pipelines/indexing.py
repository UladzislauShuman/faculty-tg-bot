import os
import pickle
import requests
import textwrap
from typing import List, Dict, Any, Set
from urllib.parse import urljoin, urlparse
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from bs4 import BeautifulSoup
from rank_bm25 import BM25Okapi
from src.strategies.chunkers import SemanticHTMLChunker

class WebsiteCrawler:
  """
  –ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ö–æ–¥–∞ –≤–µ–±-—Å–∞–π—Ç–∞, —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞.
  """

  def __init__(self, base_url: str, max_depth: int = 2):
    parsed_url = urlparse(base_url)
    self.base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    self.domain = parsed_url.netloc
    self.visited_urls: Set[str] = set()
    self.max_depth = max_depth
    self.headers = {
      "User-Agent": "Mozilla/5.0 (Windows NT 1.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

  def _is_valid_url(self, url: str) -> bool:
    parsed_url = urlparse(url)
    if parsed_url.scheme not in ['http', 'https']: return False
    if parsed_url.netloc and parsed_url.netloc != self.domain: return False
    if any(url.endswith(ext) for ext in
           ['.pdf', '.jpg', '.png', '.zip', '.docx', '.xlsx', '.pptx', '.mp4',
            '.doc']): return False
    if '/admin' in parsed_url.path or '/edit' in parsed_url.path or '/feed' in parsed_url.path: return False
    return True

  def crawl(self) -> List[Document]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ö–æ–¥–∞ —Å–∞–π—Ç–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –≥–ª—É–±–∏–Ω–µ.
    """
    urls_to_visit = [(self.base_url, 0)]
    all_documents: List[Document] = []
    while urls_to_visit:
      current_url, current_depth = urls_to_visit.pop(0)
      current_url = urljoin(current_url, urlparse(current_url).path)
      if current_url in self.visited_urls or current_depth > self.max_depth:
        if current_depth > self.max_depth: print(
          f"üåÄ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ ({self.max_depth}). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: {current_url}")
        continue
      print(f"üï∏Ô∏è  [–ì–ª—É–±–∏–Ω–∞ {current_depth}] –û–±—Ö–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {current_url}")
      self.visited_urls.add(current_url)
      try:
        response = requests.get(current_url, headers=self.headers, verify=False,
                                timeout=10)
        response.raise_for_status()
      except requests.RequestException as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {current_url}: {e}")
        continue
      raw_document = Document(page_content=response.text,
                              metadata={"source": current_url})
      all_documents.append(raw_document)
      soup = BeautifulSoup(response.text, 'lxml')
      for link in soup.find_all('a', href=True):
        href = link['href']
        absolute_url = urljoin(self.base_url, href)
        if self._is_valid_url(
            absolute_url) and absolute_url not in self.visited_urls:
          if absolute_url not in [item[0] for item in urls_to_visit]:
            urls_to_visit.append((absolute_url, current_depth + 1))
    print(f"\n‚úÖ –û–±—Ö–æ–¥ —Å–∞–π—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ–±—Ä–∞–Ω–æ {len(all_documents)} —Å—Ç—Ä–∞–Ω–∏—Ü.")
    return all_documents

  def _build_url_tree(self) -> Dict:
    """–°—Ç—Ä–æ–∏—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å (–¥–µ—Ä–µ–≤–æ) –∏–∑ –ø–ª–æ—Å–∫–æ–≥–æ —Å–ø–∏—Å–∫–∞ URL."""
    tree = {}
    for url in sorted(list(self.visited_urls)):
      path = urlparse(url).path
      parts = path.strip('/').split('/')
      current_level = tree
      for part in parts:
        if not part: continue
        if part not in current_level: current_level[part] = {}
        current_level = current_level[part]
    return tree

  def _print_tree_recursive(self, tree: Dict, prefix: str, file):
    keys = sorted(tree.keys())
    for i, key in enumerate(keys):
      is_last = (i == len(keys) - 1)
      connector = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
      file.write(f"{prefix}{connector}{key}\n")
      new_prefix = prefix + ("    " if is_last else "‚îÇ   ")
      self._print_tree_recursive(tree[key], new_prefix, file)

  def save_sitemap_to_file(self, config: Dict[str, Any]):
    output_dir = config['paths']['output_dir']
    sitemap_filename = config['paths']['sitemap']
    sitemap_path = os.path.join(output_dir, sitemap_filename)
    os.makedirs(output_dir, exist_ok=True)
    url_tree = self._build_url_tree()
    with open(sitemap_path, 'w', encoding='utf-8') as f:
      f.write(f"–ö–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞ –¥–ª—è: {self.base_url}\n")
      f.write(f"–í—Å–µ–≥–æ –ø–æ—Å–µ—â–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(self.visited_urls)}\n\n")
      f.write(f"{self.domain}\n")
      self._print_tree_recursive(url_tree, "", f)
    print(f"‚úÖ –ö–∞—Ä—Ç–∞ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {sitemap_path}")

def _save_chunks_to_file(chunks: List[Document], base_url: str,
    config: Dict[str, Any]):
  """
  –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.
  """
  output_dir = config['paths']['output_dir']
  output_filename = config['paths']['indexing']
  output_file_path = os.path.join(output_dir, output_filename)
  os.makedirs(output_dir, exist_ok=True)
  with open(output_file_path, "w", encoding="utf-8") as f:
    f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Å–∞–π—Ç: {base_url}\n")
    f.write(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {len(chunks)}\n")
    f.write("=" * 80 + "\n\n")
    for i, chunk in enumerate(chunks):
      f.write(f"--- –ß–ê–ù–ö #{i + 1} ---\n")
      f.write(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {chunk.metadata}\n")
      f.write("-" * 20 + "\n")
      wrapped_text = textwrap.fill(chunk.page_content, width=100)
      f.write(wrapped_text)
      f.write("\n\n" + "=" * 80 + "\n\n")
  print(f"‚úÖ –í—Å–µ —á–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {output_file_path}")


def _create_bm25_index(chunks: List[Document], config: Dict[str, Any]):
  """
  –°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞ –¥–∏—Å–∫ –∏–Ω–¥–µ–∫—Å BM25 –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
  """
  print("–°–æ–∑–¥–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞...")
  tokenized_corpus = [doc.page_content.split() for doc in chunks]
  bm25 = BM25Okapi(tokenized_corpus)
  bm25_index_path = config['retrievers']['bm25']['index_path']
  os.makedirs(os.path.dirname(bm25_index_path), exist_ok=True)
  with open(bm25_index_path, "wb") as f:
    pickle.dump({'bm25': bm25, 'docs': chunks}, f)
  print(f"‚úÖ BM25 –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {bm25_index_path}")


def _create_vector_store(chunks: List[Document], config: Dict[str, Any]):
  """
  –°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞ –¥–∏—Å–∫ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (ChromaDB).
  """
  print("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
  db_dir = config['retrievers']['vector_store']['db_path']
  model_name = config['embedding_model']['name']
  device = config['embedding_model']['device']
  model_kwargs = {'device': device}
  encode_kwargs = {'normalize_embeddings': True}
  embeddings_model = HuggingFaceEmbeddings(model_name=model_name,
                                           model_kwargs=model_kwargs,
                                           encode_kwargs=encode_kwargs)
  if "e5" in model_name:
    print("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å e5. –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å 'passage: ' –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.")
    e5_chunks = [Document(page_content=f"passage: {chunk.page_content}",
                          metadata=chunk.metadata) for chunk in chunks]
    chunks_to_embed = e5_chunks
  else:
    chunks_to_embed = chunks
  db = Chroma.from_documents(chunks_to_embed, embeddings_model,
                             persist_directory=db_dir)
  print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {db_dir}")


def _process_and_index_documents(raw_documents: List[Document],
    config: Dict[str, Any]):
  """
  –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—â—É—é –ª–æ–≥–∏–∫—É: —á–∞–Ω–∫–∏–Ω–≥, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤.
  """
  base_url = config['data_source']['url']

  if not raw_documents:
    print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
    return

  # –®–∞–≥ 2: –ß–∞–Ω–∫–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Å–ø–∏—Å–∫–∞
  chunker = SemanticHTMLChunker()
  all_chunks = []
  for doc in raw_documents:
    chunks_from_doc = chunker.chunk(doc)
    all_chunks.extend(chunks_from_doc)
  print(f"‚úÖ –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {len(all_chunks)} —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.")

  if not all_chunks:
    print("–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è.")
    return
  _save_chunks_to_file(all_chunks, base_url, config)

  # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ BM25 –ø–æ –í–°–ï–ú —á–∞–Ω–∫–∞–º
  _create_bm25_index(all_chunks, config)

  # –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –ø–æ –í–°–ï–ú —á–∞–Ω–∫–∞–º
  _create_vector_store(all_chunks, config)

  print("\nüéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


def run_indexing(config: Dict[str, Any]):
  """
  –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è-–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.
  –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ä–µ–∂–∏–º–∞: 'full' –∏ 'test'.
  """
  indexing_mode = config.get('indexing_mode', 'full')
  sitemap_only = config['data_source'].get('sitemap_only', False)

  if sitemap_only:
    print("\n‚úÖ –†–µ–∂–∏–º 'sitemap_only' –∞–∫—Ç–∏–≤–µ–Ω. –ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞.")
    base_url = config['data_source']['url']
    max_depth = config['data_source'].get('max_depth', 2)
    crawler = WebsiteCrawler(base_url, max_depth=max_depth)
    crawler.crawl()
    crawler.save_sitemap_to_file(config)
    print("\n‚úÖ –ö–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞. –ü—Ä–æ—Ü–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø—Ä–æ–ø—É—â–µ–Ω.")
    return

  if indexing_mode == 'test':
    print("\n‚ö°Ô∏è –ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ —Ä–µ–∂–∏–º–µ 'test' –ø–æ —Å–ø–∏—Å–∫—É URL...")
    test_urls = config['data_source'].get('test_urls', [])
    if not test_urls:
      print(
        "‚ùå –û—à–∏–±–∫–∞: –†–µ–∂–∏–º 'test' –≤—ã–±—Ä–∞–Ω, –Ω–æ —Å–ø–∏—Å–æ–∫ 'test_urls' –≤ config.yaml –ø—É—Å—Ç.")
      return

    raw_documents: List[Document] = []
    html_sources: List[str] = []
    headers = {"User-Agent": "Mozilla/5.0"}

    for url in test_urls:
      print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
      try:
        response = requests.get(url, headers=headers, verify=False, timeout=10)
        response.raise_for_status()
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        raw_documents.append(
          Document(page_content=response.text, metadata={"source": url}))
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        html_sources.append(f"###<{url}>###\n{response.text}")
      except requests.RequestException as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}: {e}")
        continue

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –≤ —Ñ–∞–π–ª
    output_dir = config['paths']['output_dir']
    source_filename = config['paths']['test_pages_source']
    source_filepath = os.path.join(output_dir, source_filename)
    os.makedirs(output_dir, exist_ok=True)
    with open(source_filepath, 'w', encoding='utf-8') as f:
      f.write('\n\n'.join(html_sources))
    print(f"‚úÖ –ò—Å—Ö–æ–¥–Ω—ã–π HTML-–∫–æ–¥ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {source_filepath}")

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å (—á–∞–Ω–∫–∏–Ω–≥, –≤–µ–∫—Ç–æ—Ä—ã, bm25) –¥–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    _process_and_index_documents(raw_documents, config)

  else:  # –†–µ–∂–∏–º 'full'
    print("\nüöÄ –ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ —Ä–µ–∂–∏–º–µ 'full' (–ø–æ–ª–Ω—ã–π –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞)...")
    base_url = config['data_source']['url']
    max_depth = config['data_source'].get('max_depth', 2)
    crawler = WebsiteCrawler(base_url, max_depth=max_depth)
    raw_documents = crawler.crawl()
    crawler.save_sitemap_to_file(config)

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    _process_and_index_documents(raw_documents, config)