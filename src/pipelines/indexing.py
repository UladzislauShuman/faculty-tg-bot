import os
import pickle
import requests
import textwrap
from typing import List, Dict, Any
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# BM25Okapi - —ç—Ç–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
# –û–Ω —Å–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã,
# —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç–µ –∂–µ —Å–ª–æ–≤–∞, —á—Ç–æ –∏ –≤ –∑–∞–ø—Ä–æ—Å–µ.
from rank_bm25 import BM25Okapi
from src.strategies.chunkers import HTMLContextChunker


def _load_raw_document(url: str) -> Document:
  """
  –ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML-—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ URL
  –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ –≤ –≤–∏–¥–µ –µ–¥–∏–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ Document.
  """
  print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
  try:
    response = requests.get(url, verify=False, timeout=10)
    response.raise_for_status()
    return Document(page_content=response.text, metadata={"source": url})
  except requests.RequestException as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ URL {url}: {e}")
    return None


def _save_chunks_to_file(chunks: List[Document], url: str,
    config: Dict[str, Any]):
  """
  –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.
  """
  output_dir = config['paths']['output_dir']
  output_filename = config['paths']['indexing']
  output_file_path = os.path.join(output_dir, output_filename)

  os.makedirs(output_dir, exist_ok=True)

  with open(output_file_path, "w", encoding="utf-8") as f:
    f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: {url}\n")
    f.write(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}\n")
    f.write("=" * 80 + "\n\n")

    for i, chunk in enumerate(chunks):
      f.write(f"--- –ß–ê–ù–ö #{i + 1} ---\n")
      f.write(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {chunk.metadata}\n")
      f.write("-" * 20 + "\n")
      wrapped_text = textwrap.fill(chunk.page_content, width=100)
      f.write(wrapped_text)
      f.write("\n\n" + "=" * 80 + "\n\n")

  print(f"‚úÖ –í—Å–µ —á–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {output_file_path}")


def _create_bm25_index(chunks: List[Document], config: Dict[str, Any]):
  """
  –°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞ –¥–∏—Å–∫ –∏–Ω–¥–µ–∫—Å BM25 –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
  """
  print("–°–æ–∑–¥–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞...")
  # 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: —Ä–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ –Ω–∞ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤.
  tokenized_corpus = [doc.page_content.split() for doc in chunks]

  # 2. –û–±—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞: —Å–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç BM25Okapi, –ø–µ—Ä–µ–¥–∞–≤–∞—è –µ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å.
  bm25 = BM25Okapi(tokenized_corpus)

  # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: –º—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Å–∞–º –æ–±—É—á–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç bm25, —Ç–∞–∫ –∏
  #    —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–∏–º –¥–æ–∫—É–º–µ–Ω—Ç—ã (chunks), —á—Ç–æ–±—ã —Ä–µ—Ç—Ä–∏–≤–µ—Ä –º–æ–≥ –∏—Ö –≤–µ—Ä–Ω—É—Ç—å.
  bm25_index_path = config['retrievers']['bm25']['index_path']
  os.makedirs(os.path.dirname(bm25_index_path), exist_ok=True)
  with open(bm25_index_path, "wb") as f:
    pickle.dump({'bm25': bm25, 'docs': chunks}, f)

  print(f"‚úÖ BM25 –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {bm25_index_path}")


def _create_vector_store(chunks: List[Document], config: Dict[str, Any]):
  """
  –°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞ –¥–∏—Å–∫ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (ChromaDB).
  """
  print("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
  db_dir = config['retrievers']['vector_store']['db_path']
  model_name = config['embedding_model']['name']
  device = config['embedding_model']['device']

  # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embedding-–º–æ–¥–µ–ª–∏.
  # HuggingFaceEmbeddings - —ç—Ç–æ –∫–ª–∞—Å—Å-–æ–±–µ—Ä—Ç–∫–∞ –≤ LangChain, –∫–æ—Ç–æ—Ä—ã–π
  # —É–ø—Ä–æ—â–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ª—é–±–æ–π embedding-–º–æ–¥–µ–ª–∏ —Å —Å–∞–π—Ç–∞ Hugging Face.
  embeddings_model = HuggingFaceEmbeddings(model_name=model_name,
                                           model_kwargs={'device': device})

  # 2. –°–æ–∑–¥–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞–∑—ã.
  # Chroma.from_documents - —ç—Ç–æ —É–¥–æ–±–Ω—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –¥–µ–ª–∞–µ—Ç –≤—Å–µ –∑–∞ –Ω–∞—Å:
  # - –ü—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É.
  # - –í—ã–∑—ã–≤–∞–µ—Ç embeddings_model –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞.
  # - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –≤–µ–∫—Ç–æ—Ä –≤ –±–∞–∑—É –Ω–∞ –¥–∏—Å–∫–µ (persist_directory).
  db = Chroma.from_documents(chunks, embeddings_model, persist_directory=db_dir)

  print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {db_dir}")


def run_indexing(config: Dict[str, Any]):
  """
  –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è-–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.
  """
  # –ó–∞–≥—Ä—É–∑–∫–∞ "—Å—ã—Ä–æ–≥–æ" –¥–æ–∫—É–º–µ–Ω—Ç–∞
  url = config['data_source']['url']
  raw_document = _load_raw_document(url)
  if not raw_document:
    return

  # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
  chunker = HTMLContextChunker()
  chunks = chunker.chunk(raw_document)
  print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ {len(chunks)} —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.")
  if not chunks:
    print("–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
    return

  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ —Ñ–∞–π–ª
  _save_chunks_to_file(chunks, url, config)

  # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
  _create_bm25_index(chunks, config)

  # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
  _create_vector_store(chunks, config)

  print("\nüéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")