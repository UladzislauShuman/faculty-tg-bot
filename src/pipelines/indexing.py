# src/pipelines/indexing.py

import os
import pickle
import requests
import textwrap 
from langchain_core.documents import Document
from rank_bm25 import BM25Okapi
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

from src.strategies.chunkers import HTMLContextChunker

def run_indexing(config: dict):
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: –∑–∞–≥—Ä—É–∑–∫–∞, —á–∞–Ω–∫–∏–Ω–≥, —Å–æ–∑–¥–∞–Ω–∏–µ –¥–≤—É—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ txt."""
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥
    url = config['data_source']['url']
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
    response = requests.get(url, verify=False)
    raw_document = Document(page_content=response.text, metadata={"source": url})

    # 2. –ì–∏–±—Ä–∏–¥–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥
    chunker = HTMLContextChunker()
    chunks = chunker.chunk(raw_document)
    print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ {len(chunks)} —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.")

    if not chunks:
        print("–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
        return

    output_dir = config['paths']['output_dir']
    output_filename = config['paths']['indexing']
    output_file_path = os.path.join(output_dir, output_filename)
    os.makedirs(output_dir, exist_ok=True)
    
    with open(output_file_path, "w", encoding="utf-8") as f:
        f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: {url}\n")
        f.write(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}\n")
        f.write("="*80 + "\n\n")

        for i, chunk in enumerate(chunks):
            f.write(f"--- –ß–ê–ù–ö #{i+1} ---\n")
            f.write(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {chunk.metadata}\n")
            f.write("-" * 20 + "\n")
            wrapped_text = textwrap.fill(chunk.page_content, width=100)
            f.write(wrapped_text)
            f.write("\n\n" + "="*80 + "\n\n")
            
    print(f"‚úÖ –í—Å–µ —á–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {output_file_path}")

    # 3. –°–æ–∑–¥–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞
    tokenized_corpus = [doc.page_content.split() for doc in chunks]
    bm25 = BM25Okapi(tokenized_corpus)
    bm25_index_path = config['retrievers']['bm25']['index_path']
    os.makedirs(os.path.dirname(bm25_index_path), exist_ok=True)
    with open(bm25_index_path, "wb") as f:
        pickle.dump({'bm25': bm25, 'docs': chunks}, f)
    print(f"‚úÖ BM25 –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {bm25_index_path}")

    # 4. –°–æ–∑–¥–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
    db_dir = config['retrievers']['vector_store']['db_path']
    model_name = config['embedding_model']['name']
    device = config['embedding_model']['device']
    
    embeddings_model = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': device})
    db = Chroma.from_documents(chunks, embeddings_model, persist_directory=db_dir)
    print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {db_dir}")

    print("\nüéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")