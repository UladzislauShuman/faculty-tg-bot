import os
import pickle
import requests
import textwrap
from typing import List, Dict, Any, Set
from urllib.parse import urljoin, urlparse
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

from bs4 import BeautifulSoup
from rank_bm25 import BM25Okapi
from src.strategies.chunkers import HTMLContextChunker

class WebsiteCrawler:
  """
  –ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ö–æ–¥–∞ –≤–µ–±-—Å–∞–π—Ç–∞, —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞—Ä—Ç—ã —Å–∞–π—Ç–∞.
  """

  def __init__(self, base_url: str, max_depth: int = 2):
    parsed_url = urlparse(base_url)
    self.base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    self.domain = parsed_url.netloc
    self.visited_urls: Set[str] = set()
    self.max_depth = max_depth
    self.headers = {
      "User-Agent": "Mozilla/5.0 (Windows NT 1.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

  def _is_valid_url(self, url: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∏ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–Ω–µ —Ñ–∞–π–ª, –Ω–µ —è–∫–æ—Ä—å).
    """
    parsed_url = urlparse(url)
    if parsed_url.scheme not in ['http', 'https']:
      return False
    if parsed_url.netloc and parsed_url.netloc != self.domain:
      return False
    if any(url.endswith(ext) for ext in
           ['.pdf', '.jpg', '.png', '.zip', '.docx', '.xlsx', '.pptx', '.mp4',
            '.doc']):
      return False
    if '/admin' in parsed_url.path or '/edit' in parsed_url.path or '/feed' in parsed_url.path:
      return False
    return True

  def crawl(self) -> List[Document]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ö–æ–¥–∞ —Å–∞–π—Ç–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –≥–ª—É–±–∏–Ω–µ.
    """
    urls_to_visit = [(self.base_url, 0)]
    all_documents: List[Document] = []

    while urls_to_visit:
      current_url, current_depth = urls_to_visit.pop(0)
      current_url = urljoin(current_url, urlparse(current_url).path)

      if current_url in self.visited_urls or current_depth > self.max_depth:
        if current_depth > self.max_depth:
          print(
            f"üåÄ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ ({self.max_depth}). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: {current_url}")
        continue

      print(f"üï∏Ô∏è  [–ì–ª—É–±–∏–Ω–∞ {current_depth}] –û–±—Ö–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {current_url}")
      self.visited_urls.add(current_url)

      try:
        response = requests.get(current_url, headers=self.headers, verify=False,
                                timeout=10)
        response.raise_for_status()
      except requests.RequestException as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {current_url}: {e}")
        continue

      raw_document = Document(page_content=response.text,
                              metadata={"source": current_url})
      all_documents.append(raw_document)

      soup = BeautifulSoup(response.text, 'lxml')
      for link in soup.find_all('a', href=True):
        href = link['href']
        absolute_url = urljoin(self.base_url, href)

        if self._is_valid_url(
            absolute_url) and absolute_url not in self.visited_urls:
          if absolute_url not in [item[0] for item in urls_to_visit]:
            urls_to_visit.append((absolute_url, current_depth + 1))

    print(f"\n‚úÖ –û–±—Ö–æ–¥ —Å–∞–π—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ–±—Ä–∞–Ω–æ {len(all_documents)} —Å—Ç—Ä–∞–Ω–∏—Ü.")
    return all_documents

  def _build_url_tree(self) -> Dict:
    """–°—Ç—Ä–æ–∏—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å (–¥–µ—Ä–µ–≤–æ) –∏–∑ –ø–ª–æ—Å–∫–æ–≥–æ —Å–ø–∏—Å–∫–∞ URL."""
    tree = {}
    for url in sorted(list(self.visited_urls)):
      path = urlparse(url).path
      parts = path.strip('/').split('/')

      current_level = tree
      for part in parts:
        if not part: continue
        if part not in current_level:
          current_level[part] = {}
        current_level = current_level[part]
    return tree

  def _print_tree_recursive(self, tree: Dict, prefix: str, file):
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–µ—á–∞—Ç–∞–µ—Ç –¥–µ—Ä–µ–≤–æ URL –≤ —Ñ–∞–π–ª, —Å–æ–∑–¥–∞–≤–∞—è –∫—Ä–∞—Å–∏–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É."""
    keys = sorted(tree.keys())
    for i, key in enumerate(keys):
      is_last = (i == len(keys) - 1)
      connector = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
      file.write(f"{prefix}{connector}{key}\n")

      new_prefix = prefix + ("    " if is_last else "‚îÇ   ")
      self._print_tree_recursive(tree[key], new_prefix, file)

  def save_sitemap_to_file(self, config: Dict[str, Any]):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–µ—Ä–µ–≤–æ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª."""
    output_dir = config['paths']['output_dir']
    sitemap_filename = config['paths']['sitemap']
    sitemap_path = os.path.join(output_dir, sitemap_filename)
    os.makedirs(output_dir, exist_ok=True)

    url_tree = self._build_url_tree()

    with open(sitemap_path, 'w', encoding='utf-8') as f:
      f.write(f"–ö–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞ –¥–ª—è: {self.base_url}\n")
      f.write(f"–í—Å–µ–≥–æ –ø–æ—Å–µ—â–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(self.visited_urls)}\n\n")
      f.write(f"{self.domain}\n")
      self._print_tree_recursive(url_tree, "", f)

    print(f"‚úÖ –ö–∞—Ä—Ç–∞ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {sitemap_path}")

def _save_chunks_to_file(chunks: List[Document], base_url: str,
    config: Dict[str, Any]):
  """
  –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.
  """
  output_dir = config['paths']['output_dir']
  output_filename = config['paths']['indexing']
  output_file_path = os.path.join(output_dir, output_filename)

  os.makedirs(output_dir, exist_ok=True)

  with open(output_file_path, "w", encoding="utf-8") as f:
    f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Å–∞–π—Ç: {base_url}\n")
    f.write(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {len(chunks)}\n")
    f.write("=" * 80 + "\n\n")

    for i, chunk in enumerate(chunks):
      f.write(f"--- –ß–ê–ù–ö #{i + 1} ---\n")
      f.write(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {chunk.metadata}\n")
      f.write("-" * 20 + "\n")
      wrapped_text = textwrap.fill(chunk.page_content, width=100)
      f.write(wrapped_text)
      f.write("\n\n" + "=" * 80 + "\n\n")

  print(f"‚úÖ –í—Å–µ —á–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {output_file_path}")


def _create_bm25_index(chunks: List[Document], config: Dict[str, Any]):
  """
  –°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞ –¥–∏—Å–∫ –∏–Ω–¥–µ–∫—Å BM25 –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
  """
  print("–°–æ–∑–¥–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞...")
  tokenized_corpus = [doc.page_content.split() for doc in chunks]
  bm25 = BM25Okapi(tokenized_corpus)

  bm25_index_path = config['retrievers']['bm25']['index_path']
  os.makedirs(os.path.dirname(bm25_index_path), exist_ok=True)
  with open(bm25_index_path, "wb") as f:
    pickle.dump({'bm25': bm25, 'docs': chunks}, f)

  print(f"‚úÖ BM25 –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {bm25_index_path}")


def _create_vector_store(chunks: List[Document], config: Dict[str, Any]):
  """
  –°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞ –¥–∏—Å–∫ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (ChromaDB).
  """
  print("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
  db_dir = config['retrievers']['vector_store']['db_path']
  model_name = config['embedding_model']['name']
  device = config['embedding_model']['device']

  embeddings_model = HuggingFaceEmbeddings(model_name=model_name,
                                           model_kwargs={'device': device})
  db = Chroma.from_documents(chunks, embeddings_model, persist_directory=db_dir)

  print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {db_dir}")


def run_indexing(config: Dict[str, Any]):
  """
  –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è-–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤—Å–µ–≥–æ —Å–∞–π—Ç–∞.
  """
  base_url = config['data_source']['url']
  max_depth = config['data_source'].get('max_depth', 2)
  sitemap_only = config['data_source'].get('sitemap_only', False)

  # –®–∞–≥ 1: –û–±—Ö–æ–¥ —Å–∞–π—Ç–∞ –∏ —Å–±–æ—Ä –≤—Å–µ—Ö "—Å—ã—Ä—ã—Ö" –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
  crawler = WebsiteCrawler(base_url, max_depth=max_depth)
  raw_documents = crawler.crawl()

  # –®–∞–≥ 1.1: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞—Ä—Ç—É —Å–∞–π—Ç–∞ –ø–æ—Å–ª–µ –æ–±—Ö–æ–¥–∞
  crawler.save_sitemap_to_file(config)

  if sitemap_only:
    print("\n‚úÖ –†–µ–∂–∏–º 'sitemap_only' –∞–∫—Ç–∏–≤–µ–Ω. –ü—Ä–æ—Ü–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø—Ä–æ–ø—É—â–µ–Ω.")
    return

  if not raw_documents:
    print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
    return

  # –®–∞–≥ 2: –ß–∞–Ω–∫–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Å–ø–∏—Å–∫–∞
  chunker = HTMLContextChunker()
  all_chunks = []
  for doc in raw_documents:
    chunks_from_doc = chunker.chunk(doc)
    all_chunks.extend(chunks_from_doc)
  print(f"‚úÖ –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {len(all_chunks)} —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.")

  if not all_chunks:
    print("–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è.")
    return
  _save_chunks_to_file(all_chunks, base_url, config)

  # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ BM25 –ø–æ –í–°–ï–ú —á–∞–Ω–∫–∞–º
  _create_bm25_index(all_chunks, config)

  # –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –ø–æ –í–°–ï–ú —á–∞–Ω–∫–∞–º
  _create_vector_store(all_chunks, config)

  print("\nüéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ–≥–æ —Å–∞–π—Ç–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")