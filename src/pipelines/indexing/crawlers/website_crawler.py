import requests
from typing import List, Set
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup


class WebsiteCrawler:
  """
  –ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ö–æ–¥–∞ –≤–µ–±-—Å–∞–π—Ç–∞ –∏ —Å–±–æ—Ä–∞ URL-–∞–¥—Ä–µ—Å–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü.
  –ï–≥–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ—Å—Ç–∞–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.
  –û–Ω –Ω–µ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –ø–∞—Ä—Å–∏–Ω–≥–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
  """

  def __init__(self, base_url: str, max_depth: int = 2):
    """
    –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –∫—Ä–∞—É–ª–µ—Ä–∞.

    Args:
        base_url (str): –ù–∞—á–∞–ª—å–Ω—ã–π URL –¥–ª—è –æ–±—Ö–æ–¥–∞.
        max_depth (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –æ–±—Ö–æ–¥–∞.
    """
    parsed_url = urlparse(base_url)
    self.base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    self.domain = parsed_url.netloc
    self.visited_urls: Set[str] = set()
    self.max_depth = max_depth
    self.headers = {
      "User-Agent": "Mozilla/5.0 (Windows NT 1.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

  def _is_valid_url(self, url: str) -> bool:
    """
    –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –≤–∞–ª–∏–¥–Ω—ã–º –¥–ª—è –æ–±—Ö–æ–¥–∞.
    –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –≤–Ω–µ—à–Ω–∏–µ —Å–∞–π—Ç—ã, —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª—ã –∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã.
    """
    parsed_url = urlparse(url)
    if parsed_url.scheme not in ['http', 'https']:
      return False
    if parsed_url.netloc and parsed_url.netloc != self.domain:
      return False
    if any(url.endswith(ext) for ext in
           ['.pdf', '.jpg', '.png', '.zip', '.docx', '.xlsx']):
      return False
    if '/admin' in parsed_url.path or '/user' in parsed_url.path:
      return False
    return True

  def crawl(self) -> List[str]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ö–æ–¥–∞ —Å–∞–π—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL.
    """
    urls_to_visit = [(self.base_url, 0)]

    while urls_to_visit:
      current_url, current_depth = urls_to_visit.pop(0)

      # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
      current_url = urljoin(current_url, urlparse(current_url).path)

      if current_url in self.visited_urls or current_depth > self.max_depth:
        continue

      print(f"üï∏Ô∏è  [–ì–ª—É–±–∏–Ω–∞ {current_depth}] –û–±—Ö–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {current_url}")
      self.visited_urls.add(current_url)

      try:
        response = requests.get(current_url, headers=self.headers, verify=False,
                                timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')

        # –ò—â–µ–º –≤—Å–µ —Ç–µ–≥–∏ <a> —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º href
        for link in soup.find_all('a', href=True):
          absolute_url = urljoin(self.base_url, link['href'])
          if self._is_valid_url(
              absolute_url) and absolute_url not in self.visited_urls:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ, –≤–∞–ª–∏–¥–Ω—ã–µ URL
            urls_to_visit.append((absolute_url, current_depth + 1))
      except requests.RequestException as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {current_url} –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫: {e}")
        continue

    print(
      f"\n‚úÖ –û–±—Ö–æ–¥ —Å–∞–π—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(self.visited_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.")
    return list(self.visited_urls)