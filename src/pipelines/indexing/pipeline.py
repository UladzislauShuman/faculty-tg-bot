import os
import pickle
import textwrap
import hashlib
from typing import List, Dict, Any

from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from rank_bm25 import BM25Okapi

from src.interfaces.data_processor_interfaces import DataSourceProcessor
from src.pipelines.indexing.crawlers.website_crawler import WebsiteCrawler


# --- –ë–õ–û–ö –£–¢–ò–õ–ò–¢–ê–†–ù–´–• –§–£–ù–ö–¶–ò–ô ---
# –≠—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —è–≤–ª—è—é—Ç—Å—è "–ø–æ–º–æ—â–Ω–∏–∫–∞–º–∏". –û–Ω–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ,
# –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –Ω–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∏.

def _save_chunks_to_file(chunks: List[Document], config: Dict[str, Any]):
  """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
  output_dir = config['paths']['output_dir']
  output_filename = config['paths']['indexing']
  output_file_path = os.path.join(output_dir, output_filename)
  os.makedirs(output_dir, exist_ok=True)
  with open(output_file_path, "w", encoding="utf-8") as f:
    f.write(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {len(chunks)}\n")
    f.write("=" * 80 + "\n\n")
    for i, chunk in enumerate(chunks):
      f.write(f"--- –ß–ê–ù–ö #{i + 1} ---\n")
      f.write(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {chunk.metadata}\n")
      f.write("-" * 20 + "\n")
      wrapped_text = textwrap.fill(chunk.page_content, width=100)
      f.write(wrapped_text)
      f.write("\n\n" + "=" * 80 + "\n\n")
  print(f"‚úÖ –í—Å–µ —á–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {output_file_path}")


def _create_bm25_index(chunks: List[Document], config: Dict[str, Any]):
  """–°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞ –¥–∏—Å–∫ –∏–Ω–¥–µ–∫—Å BM25 –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º."""
  print("–°–æ–∑–¥–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞...")
  tokenized_corpus = [doc.page_content.split() for doc in chunks]
  bm25 = BM25Okapi(tokenized_corpus)
  bm25_index_path = config['retrievers']['bm25']['index_path']
  os.makedirs(os.path.dirname(bm25_index_path), exist_ok=True)
  with open(bm25_index_path, "wb") as f:
    pickle.dump({'bm25': bm25, 'docs': chunks}, f)
  print(f"‚úÖ BM25 –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {bm25_index_path}")


def _create_vector_store(chunks: List[Document], config: Dict[str, Any]):
  """–°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞ –¥–∏—Å–∫ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (ChromaDB)."""
  print("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
  db_dir = config['retrievers']['vector_store']['db_path']
  model_name = config['embedding_model']['name']
  device = config['embedding_model']['device']
  embeddings_model = HuggingFaceEmbeddings(
      model_name=model_name,
      model_kwargs={'device': device},
      encode_kwargs={'normalize_embeddings': True}
  )

  # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π e5, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –ø—Ä–µ—Ñ–∏–∫—Å "passage:"
  if "e5" in model_name:
    chunks_to_embed = [
      Document(page_content=f"passage: {chunk.page_content}",
               metadata=chunk.metadata)
      for chunk in chunks
    ]
  else:
    chunks_to_embed = chunks

  db = Chroma.from_documents(chunks_to_embed, embeddings_model,
                             persist_directory=db_dir)
  print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {db_dir}")


# --- –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø-–û–†–ö–ï–°–¢–†–ê–¢–û–† ---
def run_indexing(config: dict, processor: DataSourceProcessor, mode: str):
  """
  –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è-–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.

  Args:
      config (dict): –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏–∑ config.yaml.
      processor (DataSourceProcessor): –û–±—ä–µ–∫—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
      mode (str): –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã - 'full' –∏–ª–∏ 'test'.
  """
  urls_to_process = []

  # --- –õ–û–ì–ò–ö–ê –í–´–ë–û–†–ê URL –í –ó–ê–í–ò–°–ò–ú–û–°–¢–ò –û–¢ –†–ï–ñ–ò–ú–ê ---
  if mode == 'test':
    print("--- –†–µ–∂–∏–º 'test': –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è URL –∏–∑ `test_urls` –≤ config.yaml ---")
    urls_to_process = config['data_source'].get('test_urls', [])
    if not urls_to_process:
      print("‚ùå –û—à–∏–±–∫–∞: –°–ø–∏—Å–æ–∫ 'test_urls' –≤ config.yaml –ø—É—Å—Ç.")
      return
  elif mode == 'full':
    print("--- –†–µ–∂–∏–º 'full': –∑–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –æ–±—Ö–æ–¥–∞ —Å–∞–π—Ç–∞ ---")
    base_url = config['data_source']['url']
    max_depth = config['data_source'].get('max_depth', 2)
    crawler = WebsiteCrawler(base_url=base_url, max_depth=max_depth)
    urls_to_process = crawler.crawl()

  # --- –û–ë–©–ê–Ø –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò –ò –ò–ù–î–ï–ö–°–ê–¶–ò–ò ---
  all_chunks = []
  for url in urls_to_process:
    chunks_from_url = processor.process(url)
    all_chunks.extend(chunks_from_url)

  print(
    f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü–æ–ª—É—á–µ–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤.")

  if not all_chunks:
    print("–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏. –ü—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
    return

  print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞...")
  for doc in all_chunks:
    unique_string = f"{doc.metadata['source']}{doc.page_content}"
    chunk_id = hashlib.md5(unique_string.encode('utf-8')).hexdigest()
    doc.metadata['chunk_id'] = chunk_id
  print("‚úÖ ID —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")

  _save_chunks_to_file(all_chunks, config)
  _create_bm25_index(all_chunks, config)
  _create_vector_store(all_chunks, config)

  print("\nüéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")